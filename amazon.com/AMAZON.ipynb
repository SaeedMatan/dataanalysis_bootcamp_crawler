{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4273827",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Project : web scraper code which extracts product's search result from Amazon.com website\n",
    "\n",
    "input --> search_text (the product name which is going to be searched in Amazon.com )\n",
    "\n",
    "output --> a csv file of desc, price, review_rating, review_count  of the desired product\n",
    "\n",
    "by Morteza Azh\n",
    "\n",
    "version 001\n",
    "\n",
    "2023-01-25\n",
    "\n",
    "'''\n",
    "\n",
    "# required libraries \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def url_generator(search_text): \n",
    "    \n",
    "    '''\n",
    "     Navigating to Amazon's website through generating a URL based on the term which is going to be searched.\n",
    "     This function will insert the search term using string formatting to the url.\n",
    "    '''\n",
    "    \n",
    "    url_format = 'https://www.amazon.com/s?k={}&crid=3W08ZKWQWWA88&sprefix=%2Caps%2C796&ref=nb_sb_ss_recent_1_0_recent'\n",
    "    \n",
    "    # Replacing the spaces in search item with '+' to conform to the URL convention\n",
    "    \n",
    "    search_text = search_text.replace(' ', '+')\n",
    "    url = url_format.format(search_text)\n",
    "    \n",
    "    # Adding a place to insert <the page number> using string formatting \n",
    "    \n",
    "    url = url + '&page{}'\n",
    "    \n",
    "    return url\n",
    "\n",
    "\n",
    "def content_extraction(row):\n",
    "    \n",
    "    ### Extracting the contents of the page from the HTML\n",
    "    \n",
    "    # Exctrating the record header or the description of the product\n",
    "    \n",
    "    a_tag = row.h2.a\n",
    "    \n",
    "    # The text property of the a tag is the description, we use .strip method to remove the extra space on the edges\n",
    "    \n",
    "    desc = a_tag.text.strip()\n",
    "    \n",
    "    '''\n",
    "    The a_tag has the href property which is the url, as it's not a complete URL we need to prepend it with the Amazon \n",
    "    webiste URL \n",
    "    '''\n",
    "    \n",
    "    url = 'https://www.amazon.com' + a_tag.get('href')\n",
    "    \n",
    "    #Extracting the price\n",
    "    \n",
    "    try:\n",
    "        price_parent = row.find('span','a-price')\n",
    "        price = price_parent.find('span','a-offscreen').text\n",
    "    except AttributeError:\n",
    "        return\n",
    "    \n",
    "        \n",
    "    # Extracting Reading out of five and number of reviews\n",
    "    try:\n",
    "        review_rating = row.i.text\n",
    "        review_count = row.find('span', {'class': 'a-size-base', 'dir' : 'auto'}).text\n",
    "        \n",
    "    except AttributeError:\n",
    "        \n",
    "        review_rating = ''\n",
    "        review_count =  ''\n",
    "    \n",
    "    result = (desc, price,review_rating,review_count,url)\n",
    "    return result\n",
    "\n",
    "def main(search_text):\n",
    "    \n",
    "    '''\n",
    "    this function accepts an argument of the search term and then it's going to run the search and save the result as\n",
    "    csv file\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # begin an instance of the web driver\n",
    "\n",
    "    driver_path = r\"C:\\\\Program Files (x86)\\\\chromedriver.exe\"\n",
    "\n",
    "    # for chrome:\n",
    "    \n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    \n",
    "    \n",
    "    record = []\n",
    "    records = []\n",
    "    \n",
    "    # generating a URL based on the search text\n",
    "    \n",
    "    url = url_generator(search_text)\n",
    "    \n",
    "    '''\n",
    "    Iterating over 20 pages using the query parameter in the URL for page number, any search that  is done in Amazon \n",
    "    will result in a maximum of 20 page results this means that we can add this page query to the URL\n",
    "    using string formatting then we can request the next page until we've extracted from all 20 pages \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    for page in range(1,21):\n",
    "        \n",
    "        # using the .get method of the driver and passing in the url as the argument\n",
    "        \n",
    "        driver.get(url.format(page))\n",
    "        \n",
    "        '''\n",
    "        before extracting the contents of the page from the HTML,we need to make a soup object which will parse the \n",
    "        html content from the page source\n",
    "        \n",
    "        '''\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        '''\n",
    "        'data-component-type' is a good option to identfy products records in the page, it's more specific than the class,\n",
    "         we use the soup object we created previously to extract all elements with a data-component-type of s-search-result\n",
    "         in div tag\n",
    "        '''\n",
    "        \n",
    "        result = soup.find_all('div' , {'data-component-type': 's-search-result'} )\n",
    "\n",
    "        # check to see if what is returned from the function is empty or not\n",
    "        \n",
    "        for row in result:\n",
    "            record = content_extraction(row)\n",
    "            if record:\n",
    "                records.append(record)\n",
    "    driver.close()\n",
    "    \n",
    "    # saving the extracted data to a csv file\n",
    "    '''\n",
    "    with open('extracted_data.csv' , 'w' , newline = '', encoding = 'utf-8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"desc\", \"price\",\"review_rating\",\"review_count\",\"url\"])\n",
    "        writer.writerows(records)\n",
    "     '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d68aba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main('laptop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
